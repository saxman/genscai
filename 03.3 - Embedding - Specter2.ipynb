{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648d7c9-cb1e-4f14-8e56-1a18c2f4ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e62dd-ee4d-4097-913e-a2ae4cc0035d",
   "metadata": {},
   "source": [
    "Load articles and prune ones without abstracts, since we're using the abstracts for generating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e8f57-7526-48ed-a7ad-9d13451efd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "\n",
    "db = TinyDB('db.json')\n",
    "table = db.table('articles')\n",
    "\n",
    "articles = table.all()\n",
    "print(f'loaded {len(articles)} articles')\n",
    "\n",
    "articles = [x for x in articles if x['abstract'] != 'No abstract available.']\n",
    "print(f'retaining {len(articles)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46457603-69de-40ff-9b4a-b87edf94e4a8",
   "metadata": {},
   "source": [
    "Stage the articles so that they can easily be loaded into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4ba10-444a-435f-be13-dff032f71c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "ids = []\n",
    "\n",
    "for article in articles:\n",
    "    documents.append(article['abstract'])\n",
    "    ids.append(article['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cac16c-2d11-4647-b5fe-688f6ded3f11",
   "metadata": {},
   "source": [
    "For finding semantically related documents, we'll use Chroma (https://www.trychroma.com/), which is a lightweight vector data store. Chroma supports swappable embedding models, filtering using metadata, keyword search, and multiple distance measurements. We'll use these features for evlauating approaches to organizing papers for downstream processing (search, summarization, keyword extraction, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d402e9-0f45-4986-a767-3e6c11ddcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"vectors_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e1dbb-dbd2-490d-a440-689a8edc810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Use SPECTER2 model for automatically generating embeddings in Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba2a07-3414-4097-888c-7736b1c1c806",
   "metadata": {},
   "source": [
    "Generate embeddings using Allen AI SPECTER2 (https://huggingface.co/allenai/specter2). SPCTER2 uses the Hugging Face Adapter library for managing model extensions for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7014057-e5ec-483e-a8af-82fa3caed01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bfe63-ade2-4814-be7b-2301a3f528fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\n",
    "\n",
    "# concatenate title and abstract\n",
    "text_batch = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "# preprocess the input\n",
    "inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                   return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "output = model(**inputs)\n",
    "# take the first token in the batch as the embedding\n",
    "embeddings = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a93505-7e48-4f2c-8245-e43f81fcddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_input(tokenizer, text_batch):\n",
    "  # preprocess the input\n",
    "  inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                   return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "  output = model(**inputs)\n",
    "  # take the first token in the batch as the embedding\n",
    "  embeddings = output.last_hidden_state[:, 0, :]\n",
    "  return embeddings\n",
    "\n",
    "model.load_adapter(\"allenai/specter2_adhoc_query\", source=\"hf\", load_as=\"specter2_adhoc_query\", set_active=True)\n",
    "query = [\"Bidirectional transformers\"]\n",
    "query_embedding = embed_input(tokenizer, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088f2c2-d3d6-4fe8-a601-0de61fef951d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
