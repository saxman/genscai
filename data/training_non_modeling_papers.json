[{"doi": "http://arxiv.org/abs/2009.02833v1", "title": "A Comparison of Virtual Analog Modelling Techniques for Desktop and Embedded Implementations", "abstract": "We develop a virtual analog model of the Klon Centaur guitar pedal circuit,\ncomparing various circuit modelling techniques. The techniques analyzed include\ntraditional modelling techniques such as nodal analysis and Wave Digital\nFilters, as well as a machine learning technique using recurrent neural\nnetworks. We examine these techniques in the contexts of two use cases: an\naudio plug-in designed to be run on a consumer-grade desktop computer, and a\nguitar pedal-style effect running on an embedded device. Finally, we discuss\nthe advantages and disdvantages of each technique for modelling different\ncircuits, and targeting different platforms.", "date": "2020-09-06", "authors": "Jatin Chowdhury", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1707.01089v1", "title": "Control Flow Information Analysis in Process Model Matching Techniques", "abstract": "Online Appendix to: \"Analyzing Control Flow Information to Improve the\nEffectiveness of Process Model Matching Techniques\" by the same authors.", "date": "2017-07-04", "authors": "Christopher Klinkm\u00fcler; Ingo Weber", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2108.07462v1", "title": "A Dimension Reduction Technique for Large-scale Structured Sparse Optimization Problems with Application to Convex Clustering", "abstract": "In this paper, we propose a novel adaptive sieving (AS) technique and an\nenhanced AS (EAS) technique, which are solver independent and could accelerate\noptimization algorithms for solving large scale convex optimization problems\nwith intrinsic structured sparsity. We establish the finite convergence\nproperty of the AS technique and the EAS technique with inexact solutions of\nthe reduced subproblems. As an important application, we apply the AS technique\nand the EAS technique on the convex clustering model, which could accelerate\nthe state-of-the-art algorithm SSNAL by more than 7 times and the algorithm\nADMM by more than 14 times.", "date": "2021-08-17", "authors": "Yancheng Yuan; Tsung-Hui Chang; Defeng Sun; Kim-Chuan Toh", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2305.07646v1", "title": "An effective initial particle sampling technique for Monte Carlo reactor transient simulations", "abstract": "We propose a technique to effectively sample initial neutron and delayed\nneutron precursor particles for Monte Carlo (MC) simulations of typical\noff-critical reactor transients. The technique can be seen as an improvement,\nor alternative, to the existing ones. Similar to some existing techniques, the\nproposed sampling technique uses the standard MC criticality calculation.\nHowever, different from the others, the technique effectively produces\nuniform-weight particles around user-specified target sizes. The technique is\nimplemented into the open-source Python-based MC code MC/DC and verified\nagainst an infinite homogeneous 361-group medium problem and the 3D C5G7-TD\nbenchmark model.", "date": "2023-05-12", "authors": "Ilham Variansyah; Ryan G. McClarren", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2304.14999v1", "title": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs", "abstract": "As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.", "date": "2023-04-28", "authors": "George Pu; Anirudh Jain; Jihan Yin; Russell Kaplan", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2009.13295v1", "title": "A Diagnostic Study of Explainability Techniques for Text Classification", "abstract": "Recent developments in machine learning have introduced models that approach\nhuman performance at the cost of increased architectural complexity. Efforts to\nmake the rationales behind the models' predictions transparent have inspired an\nabundance of new explainability techniques. Provided with an already trained\nmodel, they compute saliency scores for the words of an input instance.\nHowever, there exists no definitive guide on (i) how to choose such a technique\ngiven a particular application task and model architecture, and (ii) the\nbenefits and drawbacks of using each such technique. In this paper, we develop\na comprehensive list of diagnostic properties for evaluating existing\nexplainability techniques. We then employ the proposed list to compare a set of\ndiverse explainability techniques on downstream text classification tasks and\nneural network architectures. We also compare the saliency scores assigned by\nthe explainability techniques with human annotations of salient input regions\nto find relations between a model's performance and the agreement of its\nrationales with human ones. Overall, we find that the gradient-based\nexplanations perform best across tasks and model architectures, and we present\nfurther insights into the properties of the reviewed explainability techniques.", "date": "2020-09-25", "authors": "Pepa Atanasova; Jakob Grue Simonsen; Christina Lioma; Isabelle Augenstein", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2301.12333v1", "title": "Deep Learning model integrity checking mechanism using watermarking technique", "abstract": "In response to the growing popularity of Machine Learning (ML) techniques to\nsolve problems in various industries, various malicious groups have started to\ntarget such techniques in their attack plan. However, as ML models are\nconstantly updated with continuous data, it is very hard to monitor the\nintegrity of ML models. One probable solution would be to use hashing\ntechniques. Regardless of how that would mean re-hashing the model each time\nthe model is trained on newer data which is computationally expensive and not a\nfeasible solution for ML models that are trained on continuous data. Therefore,\nin this paper, we propose a model integrity-checking mechanism that uses model\nwatermarking techniques to monitor the integrity of ML models. We then\ndemonstrate that our proposed technique can monitor the integrity of ML models\neven when the model is further trained on newer data with a low computational\ncost. Furthermore, the integrity checking mechanism can be used on Deep\nLearning models that work on complex data distributions such as Cyber-Physical\nSystem applications.", "date": "2023-01-29", "authors": "Shahinul Hoque; Farhin Farhad Riya; Jinyuan Sun", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/cmp-lg/9505002v1", "title": "New Techniques for Context Modeling", "abstract": "We introduce three new techniques for statistical language models: extension\nmodeling, nonmonotonic contexts, and the divergence heuristic. Together these\ntechniques result in language models that have few states, even fewer\nparameters, and low message entropies. For example, our techniques achieve a\nmessage entropy of 1.97 bits/char on the Brown corpus using only 89,325\nparameters. In contrast, the character 4-gram model requires more than 250\ntimes as many parameters in order to achieve a message entropy of only 2.47\nbits/char. The fact that our model performs significantly better while using\nvastly fewer parameters indicates that it is a better probability model of\nnatural language text.", "date": "1995-05-01", "authors": "Eric Sven Ristad; Robert G. Thomas", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2407.19180v1", "title": "Data Processing Techniques for Modern Multimodal Models", "abstract": "Data processing plays an significant role in current multimodal model\ntraining. In this paper. we provide an comprehensive review of common data\nprocessing techniques used in modern multimodal model training with a focus on\ndiffusion models and multimodal large language models (MLLMs). We summarized\nall techniques into four categories: data quality, data quantity, data\ndistribution and data safety. We further present our findings in the choice of\ndata process methods in different type of models. This study aims to provide\nguidance to multimodal models developers with effective data processing\ntechniques.", "date": "2024-07-27", "authors": "Yinheng Li; Han Ding; Hang Chen", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1705.03396v1", "title": "Machine Learning Techniques for Mortality Modeling", "abstract": "Various stochastic models have been proposed to estimate mortality rates. In\nthis paper we illustrate how machine learning techniques allow us to analyze\nthe quality of such mortality models. In addition, we present how these\ntechniques can be used for differentiating the different causes of death in\nmortality modeling.", "date": "2017-05-07", "authors": "Philippe Deprez; Pavel V. Shevchenko; Mario V. W\u00fcthrich", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1901.01910v1", "title": "Model Learning: A Survey on Foundation, Tools and Applications", "abstract": "The quality and correct functioning of software components embedded in\nelectronic systems are of utmost concern especially for safety and\nmission-critical systems. Model-based testing and formal verification\ntechniques can be employed to enhance the reliability of software systems.\nFormal models form the basis and are prerequisite for the application of these\ntechniques. An emerging and promising model learning technique can complement\ntesting and verification techniques by providing learned models of black box\nsystems fully automatically. This paper surveys one such state of the art\ntechnique called model learning which recently has attracted much attention of\nresearchers especially from the domains of testing and verification. This\nsurvey paper reviews and provides comparison summaries highlighting the merits\nand shortcomings of learning techniques, algorithms, and tools which form the\nbasis of model learning. This paper also surveys the successful applications of\nmodel learning technique in multidisciplinary fields making it promising for\ntesting and verification of realistic systems.", "date": "2018-12-06", "authors": "Shahbaz Ali; Hailong Sun; Yongwang Zhao", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2303.14762v1", "title": "Approaches to Improving the Accuracy of Machine Learning Models in Requirements Elicitation Techniques Selection", "abstract": "Selecting techniques is a crucial element of the business analysis approach\nplanning in IT projects. Particular attention is paid to the choice of\ntechniques for requirements elicitation. One of the promising methods for\nselecting techniques is using machine learning algorithms trained on the\npractitioners' experience considering different projects' contexts. The\neffectiveness of ML models is significantly affected by the balance of the\ntraining dataset, which is violated in the case of popular techniques. The\npaper aims to analyze the efficiency of the Synthetic Minority Over-sampling\nTechnique usage in Machine Learning models for elicitation technique selection\nin case of the imbalanced training dataset and possible ways for positive\nfeature importance selection. The computational experiment results confirmed\nthe effectiveness of using the proposed approaches to improve the accuracy of\nmachine learning models for selecting requirements elicitation techniques.\nProposed approaches can be used to build Machine Learning models for business\nanalysis activities planning in IT projects.", "date": "2023-03-26", "authors": "Denys Gobov; Olga Solovei", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2412.07430v1", "title": "Knowledge Graph Guided Evaluation of Abstention Techniques", "abstract": "To deploy language models safely, it is crucial that they abstain from\nresponding to inappropriate requests. Several prior studies test the safety\npromises of models based on their effectiveness in blocking malicious requests.\nIn this work, we focus on evaluating the underlying techniques that cause\nmodels to abstain. We create SELECT, a benchmark derived from a set of benign\nconcepts (e.g., \"rivers\") from a knowledge graph. The nature of SELECT enables\nus to isolate the effects of abstention techniques from other safety training\nprocedures, as well as evaluate their generalization and specificity. Using\nSELECT, we benchmark different abstention techniques over six open-weight and\nclosed-source models. We find that the examined techniques indeed cause models\nto abstain with over $80\\%$ abstention rates. However, these techniques are not\nas effective for descendants of the target concepts, with refusal rates\ndeclining by $19\\%$. We also characterize the generalization-vs-specificity\ntrade-offs for different techniques. Overall, no single technique is invariably\nbetter than the others. Our findings call for a careful evaluation of different\naspects of abstention, and hopefully inform practitioners of various trade-offs\ninvolved.", "date": "2024-12-10", "authors": "Kinshuk Vasisht; Navreet Kaur; Danish Pruthi", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1801.05917v1", "title": "A Large-Scale Empirical Comparison of Static and Dynamic Test Case Prioritization Techniques", "abstract": "The large body of existing research in Test Case Prioritization (TCP)\ntechniques, can be broadly classified into two categories: dynamic techniques\n(that rely on run-time execution information) and static techniques (that\noperate directly on source and test code). Absent from this current body of\nwork is a comprehensive study aimed at understanding and evaluating the static\napproaches and comparing them to dynamic approaches on a large set of projects.\n  In this work, we perform the first extensive study aimed at empirically\nevaluating four static TCP techniques comparing them with state-of-research\ndynamic TCP techniques at different test-case granularities (e.g., method and\nclass-level) in terms of effectiveness, efficiency and similarity of faults\ndetected. This study was performed on 30 real-word Java programs encompassing\n431 KLoC. In terms of effectiveness, we find that the static call-graph-based\ntechnique outperforms the other static techniques at test-class level, but the\ntopic-model-based technique performs better at test-method level. In terms of\nefficiency, the static call-graph-based technique is also the most efficient\nwhen compared to other static techniques. When examining the similarity of\nfaults detected for the four static techniques compared to the four dynamic\nones, we find that on average, the faults uncovered by these two groups of\ntechniques are quite dissimilar, with the top 10% of test cases agreeing on\nonly 25% - 30% of detected faults. This prompts further research into the\nseverity/importance of faults uncovered by these techniques, and into the\npotential for combining static and dynamic information for more effective\napproaches.", "date": "2018-01-18", "authors": "Qi Luo; Kevin Moran; Denys Poshyvanyk", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1801.10269v1", "title": "The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models", "abstract": "Defect prediction models that are trained on class imbalanced datasets (i.e.,\nthe proportion of defective and clean modules is not equally represented) are\nhighly susceptible to produce inaccurate prediction models. Prior research\ncompares the impact of class rebalancing techniques on the performance of\ndefect prediction models. Prior research efforts arrive at contradictory\nconclusions due to the use of different choice of datasets, classification\ntechniques, and performance measures. Such contradictory conclusions make it\nhard to derive practical guidelines for whether class rebalancing techniques\nshould be applied in the context of defect prediction models. In this paper, we\ninvestigate the impact of 4 popularly-used class rebalancing techniques on 10\ncommonly-used performance measures and the interpretation of defect prediction\nmodels. We also construct statistical models to better understand in which\nexperimental design settings that class rebalancing techniques are beneficial\nfor defect prediction models. Through a case study of 101 datasets that span\nacross proprietary and open-source systems, we recommend that class rebalancing\ntechniques are necessary when quality assurance teams wish to increase the\ncompleteness of identifying software defects (i.e., Recall). However, class\nrebalancing techniques should be avoided when interpreting defect prediction\nmodels. We also find that class rebalancing techniques do not impact the AUC\nmeasure. Hence, AUC should be used as a standard measure when comparing defect\nprediction models.", "date": "2018-01-31", "authors": "Chakkrit Tantithamthavorn; Ahmed E. Hassan; Kenichi Matsumoto", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1505.07267v1", "title": "Prototyping Information Visualization in 3D City Models: a Model-based Approach", "abstract": "When creating 3D city models, selecting relevant visualization techniques is\na particularly difficult user interface design task. A first obstacle is that\ncurrent geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and\nlimited sets of visualization techniques. Another important obstacle is the\nlack of unified description of information visualization techniques for 3D city\nmodels. If many techniques have been devised for different types of data or\ninformation (wind flows, air quality fields, historic or legal texts, etc.)\nthey are generally described in articles, and not really formalized. In this\npaper we address the problem of visualizing information in (rich) 3D city\nmodels by presenting a model-based approach for the rapid prototyping of\nvisualization techniques. We propose to represent visualization techniques as\nthe composition of graph transformations. We show that these transformations\ncan be specified with SPARQL construction operations over RDF graphs. These\nspecifications can then be used in a prototype generator to produce 3D scenes\nthat contain the 3D city model augmented with data represented using the\ndesired technique.", "date": "2015-05-27", "authors": "Claudine M\u00e9tral; Gilles Falquet", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2102.06237v1", "title": "An Investigation of End-to-End Models for Robust Speech Recognition", "abstract": "End-to-end models for robust automatic speech recognition (ASR) have not been\nsufficiently well-explored in prior work. With end-to-end models, one could\nchoose to preprocess the input speech using speech enhancement techniques and\ntrain the model using enhanced speech. Another alternative is to pass the noisy\nspeech as input and modify the model architecture to adapt to noisy speech. A\nsystematic comparison of these two approaches for end-to-end robust ASR has not\nbeen attempted before. We address this gap and present a detailed comparison of\nspeech enhancement-based techniques and three different model-based adaptation\ntechniques covering data augmentation, multi-task learning, and adversarial\nlearning for robust ASR. While adversarial learning is the best-performing\ntechnique on certain noise types, it comes at the cost of degrading clean\nspeech WER. On other relatively stationary noise types, a new speech\nenhancement technique outperformed all the model-based adaptation techniques.\nThis suggests that knowledge of the underlying noise type can meaningfully\ninform the choice of adaptation technique.", "date": "2021-02-11", "authors": "Archiki Prasad; Preethi Jyothi; Rajbabu Velmurugan", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1409.5024v1", "title": "Comparative Study of Geometric and Image Based Modelling and Rendering Techniques", "abstract": "This is a comparative study of the traditional 3D computer graphics technique\nof geometric modelling and image-based rendering techniques that were surveyed\nand implemented.We have discussed the classifications and representative\nmethods of both the techniques. The study has shown that there is a strong\ncontinuum between both the techniques and a hybrid of the two is most suitable\nfor further implementations.This hybridisation study is underway to create\nmodels of real life situations and provide disaster management training.", "date": "2014-09-01", "authors": "Agrima Seth; Deepak Mishra", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/2308.00197v1", "title": "Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique", "abstract": "Vision Transformers (ViTs) have emerged as a promising approach for visual\nrecognition tasks, revolutionizing the field by leveraging the power of\ntransformer-based architectures. Among the various ViT models, Swin\nTransformers have gained considerable attention due to their hierarchical\ndesign and ability to capture both local and global visual features\neffectively. This paper evaluates the performance of Swin ViT model using\ngradient accumulation optimization (GAO) technique. We investigate the impact\nof gradient accumulation optimization technique on the model's accuracy and\ntraining time. Our experiments show that applying the GAO technique leads to a\nsignificant decrease in the accuracy of the Swin ViT model, compared to the\nstandard Swin Transformer model. Moreover, we detect a significant increase in\nthe training time of the Swin ViT model when GAO model is applied. These\nfindings suggest that applying the GAO technique may not be suitable for the\nSwin ViT model, and concern should be undertaken when using GAO technique for\nother transformer-based models.", "date": "2023-07-31", "authors": "Sanad Aburass; Osama Dorgham", "server": "arxiv"}, {"doi": "http://arxiv.org/abs/1910.00729v1", "title": "Mid-Point Technique for Calculating Divergent integrals", "abstract": "A mid-point technique is suggested to overcome the difficulties in other\ntechniques. The modified effective interaction quark potential which uses to\ncalculate different properties of the NJL model such as the constituent quark\nmass, the pressure, and the energy density is solved using the present\ntechnique. The present method gives good accuracy for the mathematical problem\nand avoids the physical difficulty in the previous works.", "date": "2019-09-30", "authors": "M. Abu-Shady", "server": "arxiv"}]