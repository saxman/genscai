{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b71b0-f8c6-4b95-b2a3-39577f8a9ffd",
   "metadata": {},
   "source": [
    "# 01 - Information Retrieval\n",
    "\n",
    "This notbook demonstrates how to extract paper abstracts from the MIDAS website and to store them locally for processing. Beautifulsoup (v4) is used for extracting information from web pages, and TinyDB is used as a simple means for storing and querying for papers.\n",
    "\n",
    "Section I in this notebook demonstrates how information can be retrieved from the MIDAS website. The code from this section is encapsulated in the genscai.retrieval module. For staging data for the other example notebooks, Section II show how the genscai.retrieval module is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c66486-4804-48a3-8fe6-be3465da8c9c",
   "metadata": {},
   "source": [
    "## Section 0 - Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864245cd-760e-400b-a1a1-815623e61192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet tinydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet ipywidgets # for displaying progress bars in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49bf3f-cda3-4726-b842-f072448c1b3a",
   "metadata": {},
   "source": [
    "## Section I - Retrieving Paper Information from the MIDAS Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aac03a",
   "metadata": {},
   "source": [
    "Define a function for retrieving a page of filtered articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62952b65-1670-4047-a37e-cfe5dc01711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "HTTP_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:133.0) Gecko/20100101 Firefox/133.0'\n",
    "}\n",
    "\n",
    "def retrieve_page(page_number):\n",
    "    url = 'https://midasnetwork.us/wp-admin/admin-ajax.php'\n",
    "\n",
    "    data = {\n",
    "        'action': 'filter_papers',\n",
    "        'paged': f'{page_number}',\n",
    "        'journal': '',\n",
    "        'author': '',\n",
    "        'title': '',\n",
    "        'startdate': '2024-11-01',\n",
    "        'enddate': '',\n",
    "        'displaydefault': '',\n",
    "    }\n",
    "\n",
    "    return requests.post(url, data=data, headers=HTTP_HEADERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e89968",
   "metadata": {},
   "source": [
    "Retrieve the first page of the paginated list of filtered articles. Determine the number of pages to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = retrieve_page(1)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "pages = soup.find_all('a', {'class', 'page-numbers'})\n",
    "\n",
    "last_page = int(pages[-2].text)\n",
    "\n",
    "print(f'pages to process: {last_page}')\n",
    "\n",
    "## TODO fails on a single page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c9050",
   "metadata": {},
   "source": [
    "Create a list of links to article detail pages by iterating over the pages of filtered articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64174c-0bcd-4b0b-80a8-3530b6e110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(f'processing page {i} of {last_page}')\n",
    "\n",
    "    for article in soup.find_all('article'):\n",
    "        link = article.find('a')\n",
    "        article_links.append(link.get('href'))\n",
    "\n",
    "    # stop if we just processed the last page\n",
    "    if i == last_page:\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    print(f'retrieving page {i}')\n",
    "\n",
    "    page = retrieve_page(i)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bafeb6",
   "metadata": {},
   "source": [
    "Load details for each article by iterating over the list of article detail page links and loading and processing each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bfcd5-3b0b-4a68-8457-6407d6d7c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=len(article_links))\n",
    "display(progress_bar)\n",
    "\n",
    "articles = []\n",
    "print(f'procesing {len(article_links)} articles')\n",
    "\n",
    "for link in article_links:\n",
    "    resp = requests.get(link, headers=HTTP_HEADERS)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    data = soup.find_all('p', {'class', 'elementor-heading-title'})\n",
    "    \n",
    "    article = {\n",
    "        'title': data[0].text,\n",
    "        'abstract': data[1].text,\n",
    "        'journal': data[2].text,\n",
    "        'reference': data[3].text,\n",
    "        'midas_authors': []\n",
    "    }\n",
    "    \n",
    "    refs = data[3].find_all('a')\n",
    "    article['link'] = refs[1].get('href')\n",
    "\n",
    "    # extract MIDAS author names from the page\n",
    "    data = soup.find_all(class_='elementor-post__title')\n",
    "    for author in data:\n",
    "        article['midas_authors'].append(author.text.strip())\n",
    "\n",
    "    articles.append(article)\n",
    "\n",
    "    progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f98b84",
   "metadata": {},
   "source": [
    "Store the article details in a local NoSQL (JSON) database for later re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eaa8cf-7bf2-410f-b896-05521bc69a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "\n",
    "db = TinyDB('db.json')\n",
    "table = db.table('articles')\n",
    "\n",
    "for article in articles:\n",
    "    table.insert(article)\n",
    "\n",
    "print(f'{len(table.all())} articles stored (total)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d1175-b4ab-4a54-8d33-f1591e6d69b2",
   "metadata": {},
   "source": [
    "## Section II - Using genscai.retrieval for Retrieving Paper Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12be0a-9317-443e-b186-5b3b429ff362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from genscai import retrieval, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab4356-6c46-4683-8da3-864262610cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieval.MIDASRetriever(\n",
    "    startdate=\"2024-01-01\",\n",
    "    enddate=\"2024-12-31\",\n",
    "    database_path=paths.data)\n",
    "\n",
    "retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19a300-c825-460d-a44b-0ec9a787c2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
