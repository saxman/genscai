{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fcb4e-94c7-4020-bb4d-02988b161ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45d717-88d9-47d8-8e20-2eddb6e7fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e62dd-ee4d-4097-913e-a2ae4cc0035d",
   "metadata": {},
   "source": [
    "Load articles and prune ones without abstracts, since we're using the abstracts for generating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e8f57-7526-48ed-a7ad-9d13451efd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "\n",
    "db = TinyDB('db.json')\n",
    "table = db.table('articles')\n",
    "\n",
    "articles = table.all()\n",
    "print(f'loaded {len(articles)} articles')\n",
    "\n",
    "articles = [x for x in articles if x['abstract'] != 'No abstract available.']\n",
    "print(f'retaining {len(articles)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46457603-69de-40ff-9b4a-b87edf94e4a8",
   "metadata": {},
   "source": [
    "Stage the articles so that they can easily be loaded into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4ba10-444a-435f-be13-dff032f71c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "ids = []\n",
    "\n",
    "for article in articles:\n",
    "    documents.append(article['abstract'])\n",
    "    ids.append(article['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cac16c-2d11-4647-b5fe-688f6ded3f11",
   "metadata": {},
   "source": [
    "For finding semantically related documents, we'll use Chroma (https://www.trychroma.com/), which is a lightweight vector data store. Chroma supports swappable embedding models, filtering using metadata, keyword search, and multiple distance measurements. We'll use these features for evlauating approaches to organizing papers for downstream processing (search, summarization, keyword extraction, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d402e9-0f45-4986-a767-3e6c11ddcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"vectors_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302d6a5-a8b8-45db-8a50-d5849ec5faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd5e15-a2c9-4036-bf54-944d7a137d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"nvidia/NV-Embed-v2\",\n",
    "    # device='cuda',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "client.delete_collection(name=\"articles-nv_embed_v2-embeddings\")\n",
    "collection = client.create_collection(\n",
    "    name=\"articles2\",\n",
    "    embedding_function=ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f883cf-f21f-456e-9ec3-57fe491936f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=len(documents))\n",
    "display(progress_bar)\n",
    "\n",
    "for i, item in enumerate(documents):\n",
    "    collection.add(\n",
    "        documents=item,\n",
    "        ids=ids[i]\n",
    "    )\n",
    "\n",
    "    progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1f7f5-b787-48a0-8874-89bee50b06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"nvidia/NV-Embed-v2\",\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# model = DataParallel(model)\n",
    "for module_key, module in model._modules.items():\n",
    "    model._modules[module_key] = DataParallel(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c979f7-3185-42fe-893d-9082eed6a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode(articles[0]['abstract'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
