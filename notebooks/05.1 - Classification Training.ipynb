{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.1 - _Training_ an LLM to Classify Disease Modeling Papers using Promptbreeder Technique\n",
    "\n",
    "https://arxiv.org/abs/2309.16797"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/modeling_papers.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_data = pd.json_normalize(data)\n",
    "\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the model classification prompt is to insruct the LLM to only return YES or NO, if the authors of the paper employ an infectious disease modeling technique or not, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PROMPT_TEMPLATE = \"\"\"\n",
    "Read the following scientific paper abstract. Based on the content, determine if the paper explicitly refers to or uses a disease modeling technique,\n",
    "including but not limited to mathematical, statistical, or computational methods used to simulate, analyze, predict, or interpret the dynamics of a disease,\n",
    "specifically in the context of estimating the probability of disease resurgence.\n",
    "\n",
    "Consider the use of disease modeling if the abstract describes or references compartmental models, statistical models, simulation models, mathematical equations,\n",
    "or functional forms to analyze or predict disease transmission, risk factors, or the effects of interventions.\n",
    "\n",
    "Additionally, if the paper uses epidemiological modeling, disease forecasting, regression analysis, or statistical analysis to investigate associations\n",
    "between disease characteristics and external factors, consider it a form of disease modeling technique.\n",
    "\n",
    "If the abstract specifically mentions estimating the probability of disease resurgence using quantitative methods, such as statistical models or mathematical equations,\n",
    "consider it a form of disease modeling technique.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PROMPT_IO_TEMPLATE = \"\"\"\n",
    "If the abstract describes or references any of these methods or similar approaches, answer \"YES\".\n",
    "If the abstract focuses on non-modeling analysis, such as reporting observational data without reference to disease modeling techniques, answer \"NO\".\n",
    "Do not include any additional text or information.\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUTATION_PROMPT_TEMPLATE = \"\"\"\n",
    "Read the prompt and scientific paper abstract below. Modify the prompt so that, if an LLM were given the modified prompt and abstract, the LLM would say that the paper explicitly refers to or uses a disease modeling technique.\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are used when a model is instantiated. These determine where and how the model is stored in memory (system/CPU or GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"device_map\": \"sequential\",  # load the model into GPUs sequentially, to avoid memory allocation issues with balancing\n",
    "    \"torch_dtype\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generator parameters are model hyperparameters used during text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the Hugging Face identifiers for the models that we'll use for classifying documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for instantiating models and using models to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def load_model(model_id, model_kwargs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor\n",
    "\n",
    "\n",
    "def generate_text(model, prompt, tokenizer, generate_kwargs):\n",
    "    generate_kwargs[\"bos_token_id\"] = tokenizer.bos_token_id\n",
    "    generate_kwargs[\"pad_token_id\"] = tokenizer.eos_token_id\n",
    "    generate_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "\n",
    "    # processor = AutoProcessor.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    # generate_kwargs[\"attention_mask\"] = processor(\"test\", return_tensors=\"pt\")['attention_mask']\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(input_ids, **generate_kwargs)\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1] :]\n",
    "\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(MODEL_ID, model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=len(df_data))\n",
    "display(progress_bar)\n",
    "\n",
    "results = {}\n",
    "is_modeling = []\n",
    "\n",
    "prompt_template = TASK_PROMPT_TEMPLATE + \"\\n\\n\" + TASK_PROMPT_IO_TEMPLATE\n",
    "\n",
    "for paper in df_data.itertuples():\n",
    "    prompt = prompt_template.format(abstract=paper.abstract)\n",
    "    result = generate_text(model, prompt, tokenizer, generate_kwargs)\n",
    "\n",
    "    if \"yes\" in result.lower():\n",
    "        is_modeling.append(True)\n",
    "    else:\n",
    "        is_modeling.append(False)\n",
    "\n",
    "    progress_bar.value += 1\n",
    "\n",
    "results[\"is_modeling\"] = is_modeling\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(is_modeling) / len(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = df_data[df_results[\"is_modeling\"] == False]\n",
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model,\n",
    "    MUTATION_PROMPT_TEMPLATE.format(\n",
    "        prompt=TASK_PROMPT_TEMPLATE, abstract=df_neg.iloc[0].abstract\n",
    "    ),\n",
    "    tokenizer,\n",
    "    generate_kwargs,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model,\n",
    "    result + \"\\nAbstract:\\n\" + df_neg.iloc[0].abstract,\n",
    "    tokenizer,\n",
    "    generate_kwargs,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
