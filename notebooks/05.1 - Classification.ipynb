{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.2 - Classification of Scientific Papers Using Open Hugging Face Models\n",
    "\n",
    "This notebook explores how open LLMs, such a Mistral, Llama, Gemma, Specter, etc., can be used for classifying scientific papers based on the content or their abstracts. Specifically, these models will be used to detect papers that discuss infectious disease modeling, and further identify which modeling techniques are used.\n",
    "\n",
    "In order to increase the accuracy of the classification, multiple models will be evaluated and employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset, which includes a column specifyting whether the paper has been manually classificed as a disease modeling paper, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genscai.data import load_classification_training_data\n",
    "\n",
    "df = load_classification_training_data()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model client for classifying papers. The model clients come in three varieties: AisuiteClient, OllamaClient, and HuggingFaceClient. Aisuite works with cloud model providers (e.g. OpenAI) as well as models hosted locally with Ollama. The Ollama client work with models hosted locally with Ollama. And the HuggingFaceClient uses the Hugging Face Transformers library for running models locally.\n",
    "\n",
    "For local models, Ollama is preferred if device memory is limited, since Ollama hosted models are typically 4-bit quantized. For greater control of quantization and model parameters, Hugging Face Transformer models are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genscai.models import HuggingFaceClient, MODEL_KWARGS\n",
    "\n",
    "client = HuggingFaceClient(HuggingFaceClient.MODEL_DEEPSEEK_R1_8B, MODEL_KWARGS)\n",
    "\n",
    "# the following only works for HuggingFaceClient since the model is hosted locally.\n",
    "client.print_model_info()\n",
    "client.print_device_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify each of the papers using the following model parameters (e.g. temperature) and prompt.\n",
    "\n",
    "For local, non-reasoning models (e.g. Llama, Gemma, Phi), we want low temperature, since we're looking for a deterministic classification. Also, we only need a single token in the response. For reasoning models, however (e.g. DeepSeek R1), we want higher temperature, to promote reasoning, and more output tokens, which includes the reasoning output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genscai.classification as gc\n",
    "\n",
    "# increase temperature (from 0.01) and max_new_tokens (from 1) to allow for longer text generation for reasoning models\n",
    "generate_kwargs = gc.CLASSIFICATION_GENERATE_KWARGS.copy()\n",
    "generate_kwargs.update(\n",
    "    {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.70,\n",
    "    }\n",
    ")\n",
    "\n",
    "df = gc.classify_papers(\n",
    "    client, gc.CLASSIFICATION_TASK_PROMPT_TEMPLATE + gc.CLASSIFICATION_OUTPUT_PROMPT_TEMPLATE, generate_kwargs, df\n",
    ")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the precision, recall, and accuracy of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, metrics = gc.test_paper_classifications(df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
