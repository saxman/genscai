{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.2 - Classification of Scientific Papers Using Open Hugging Face Models\n",
    "\n",
    "This notebook explores how open LLMs, such a Mistral, Llama, Gemma, Specter, etc., can be used for classifying scientific papers based on the content or their abstracts. Specifically, these models will be used to detect papers that discuss infectious disease modeling, and further identify which modeling techniques are used.\n",
    "\n",
    "In order to increase the accuracy of the classification, multiple models will be evaluated and employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load the training dataset, which includes a column specifyting whether the paper has been manually classificed as a disease modeling paper, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genscai.data import load_classification_training_data\n",
    "\n",
    "df = load_classification_training_data()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Set-up\n",
    "\n",
    "Create a model client for classifying papers. The model clients come in three varieties: AisuiteClient, OllamaClient, and HuggingFaceClient. Aisuite works with cloud model providers (e.g. OpenAI) as well as models hosted locally with Ollama. The Ollama client work with models hosted locally with Ollama. And the HuggingFaceClient uses the Hugging Face Transformers library for running models locally.\n",
    "\n",
    "For local models, Ollama is preferred if device memory is limited, since Ollama hosted models are typically 4-bit quantized. For greater control of quantization and model parameters, Hugging Face Transformer models are preferred."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from aimu.models import HuggingFaceClient as ModelClient\n\nmodel_kwargs = {\n    \"low_cpu_mem_usage\": True,\n    \"torch_dtype\": \"auto\",\n    \"device_map\": \"auto\",\n}\n\nclient = ModelClient(ModelClient.MODEL_GEMMA_3_12B, model_kwargs)\n\n# the following only works for HuggingFaceClient since the model is hosted locally.\nclient.print_model_info()\nclient.print_device_map()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Classification\n",
    "\n",
    "Classify each of the papers in the dataframe. The classify_papers method add a 'predict_modeling' column to the dataframe.\n",
    "\n",
    "For local, non-reasoning models (e.g. Llama, Gemma, Phi), we want low temperature, since we're looking for a deterministic classification. Also, we only need a single token in the response. For reasoning models, however (e.g. DeepSeek R1), we want higher temperature, to promote reasoning, and more output tokens, which includes the reasoning output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genscai.classification as gc\n",
    "\n",
    "generate_kwargs = gc.CLASSIFICATION_GENERATE_KWARGS.copy()\n",
    "\n",
    "# increase temperature (from 0.01) and max_new_tokens (from 1) to allow for longer text generation for reasoning models\n",
    "if client.model_id == ModelClient.MODEL_DEEPSEEK_R1_8B:\n",
    "    generate_kwargs.update(\n",
    "        {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": 0.70,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# start with the default prompt for classification tasks\n",
    "task_prompt = gc.CLASSIFICATION_TASK_PROMPT_TEMPLATE\n",
    "\n",
    "df = gc.classify_papers(client, task_prompt + gc.CLASSIFICATION_OUTPUT_PROMPT_TEMPLATE, generate_kwargs, df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Evaluation\n",
    "\n",
    "Determine the overall precision, recall, and accuracy of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, metrics = gc.test_paper_classifications(df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tuning\n",
    "\n",
    "If the accuracy wasn't 100%, see if we can have the model tune the prompt to increase it's classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genscai.training import MUTATION_NEG_PROMPT_TEMPLATE, MUTATION_POS_PROMPT_TEMPLATE, MUTATION_GENERATE_KWARGS\n",
    "\n",
    "# randomly select an incorrect result for mutating the task prompt\n",
    "df_bad = df.query(\"is_modeling != predict_modeling\")\n",
    "item = df_bad.sample().iloc[0]\n",
    "\n",
    "# generate a mutation prompt based on the incorrect result\n",
    "if item.is_modeling:\n",
    "    mutation_prompt = MUTATION_POS_PROMPT_TEMPLATE.format(prompt=task_prompt, abstract=item.abstract)\n",
    "else:\n",
    "    mutation_prompt = MUTATION_NEG_PROMPT_TEMPLATE.format(prompt=task_prompt, abstract=item.abstract)\n",
    "\n",
    "# generate a new prompt using the mutation prompt\n",
    "result = client.generate_text(mutation_prompt, MUTATION_GENERATE_KWARGS)\n",
    "task_prompt = result.split(\"<prompt>\")[-1].split(\"</prompt>\")[0].strip()\n",
    "task_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-classify Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gc.classify_papers(client, task_prompt + gc.CLASSIFICATION_OUTPUT_PROMPT_TEMPLATE, generate_kwargs, df)\n",
    "df, metrics = gc.test_paper_classifications(df)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}