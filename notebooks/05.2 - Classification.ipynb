{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.2 - Classification of Scientific Papers Using Open Hugging Face Models\n",
    "\n",
    "This notebook explores how open LLMs, such a Mistral, Llama, Gemma, Specter, etc., can be used for classifying scientific papers based on the content or their abstracts. Specifically, these models will be used to detect papers that discuss infectious disease modeling, and further identify which modeling techniques are used.\n",
    "\n",
    "In order to increase the accuracy of the classification, multiple models will be evaluated and employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paper abstracts from Hugging Face or locally if they've already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "fname = \"../data/train.json\"\n",
    "if os.path.isfile(fname):\n",
    "    df_train = pd.read_json(fname)\n",
    "else:\n",
    "    df_train = pd.read_csv(\"hf://datasets/krosenf/midas-abstracts/train.csv\")\n",
    "    df_train.to_json(fname)\n",
    "\n",
    "fname = \"../data/validate.json\"\n",
    "if os.path.isfile(fname):\n",
    "    df_validate = pd.read_json(fname)\n",
    "else:\n",
    "    df_validate = pd.read_csv(\"hf://datasets/krosenf/midas-abstracts/validate.csv\")\n",
    "    df_validate.to_json(fname)\n",
    "\n",
    "fname = \"../data/test.json\"\n",
    "if os.path.isfile(fname):\n",
    "    df_test = pd.read_json(fname)\n",
    "else:\n",
    "    df_test = pd.read_csv(\"hf://datasets/krosenf/midas-abstracts/test.csv\")\n",
    "    df_test.to_json(fname)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_validate.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "df = df_validate\n",
    "\n",
    "# df = pd.concat([df_train, df_validate, df_test])\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "\n",
    "def generate_text(model, prompt, tokenizer, generate_kwargs):\n",
    "    generate_kwargs[\"bos_token_id\"] = tokenizer.bos_token_id\n",
    "    generate_kwargs[\"pad_token_id\"] = tokenizer.eos_token_id\n",
    "    generate_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "\n",
    "    # processor = AutoProcessor.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "    # generate_kwargs[\"attention_mask\"] = processor(\"test\", return_tensors=\"pt\")['attention_mask']\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(input_ids, **generate_kwargs)\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1] :]\n",
    "\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_id in MODEL_IDS:\n",
    "    print(model_id)\n",
    "\n",
    "    progress_bar = IntProgress(min=0, max=len(df))\n",
    "    display(progress_bar)\n",
    "\n",
    "    model, tokenizer = load_model(model_id, model_kwargs)\n",
    "    is_modeling = []\n",
    "\n",
    "    for paper in df.itertuples():\n",
    "        prompt = MODEL_CLASSIFICATION_PROMPT_TEMPLATE.format(abstract=paper.abstract)\n",
    "        result = generate_text(model, prompt, tokenizer, generate_kwargs)\n",
    "\n",
    "        if \"YES\" in result:\n",
    "            is_modeling.append(True)\n",
    "        else:\n",
    "            is_modeling.append(False)\n",
    "\n",
    "        progress_bar.value += 1\n",
    "\n",
    "    results[model_id] = is_modeling\n",
    "\n",
    "    del model\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(MODEL_IDS)):\n",
    "    print(MODEL_IDS[i])\n",
    "    print(sum(df_results[MODEL_IDS[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rate = []\n",
    "\n",
    "for row in df_results.itertuples():\n",
    "    pos_rate.append(sum(row[1:]) / len(df_results.columns))\n",
    "\n",
    "df_results[\"pos_rate\"] = pos_rate\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_json(\"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.query(\"pos_rate < .5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\"infectious disease modeling\"], n_results=400)\n",
    "\n",
    "match_ids = results[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_modeling = []\n",
    "\n",
    "for i, x in df_validate.iterrows():\n",
    "    if x[\"id\"] in match_ids[0]:\n",
    "        is_modeling.append(True)\n",
    "    else:\n",
    "        is_modeling.append(False)\n",
    "\n",
    "df_results[\"chroma\"] = is_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_results[\"chroma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_modeling_agree = [\n",
    "    a and b and c and d\n",
    "    for a, b, c, d in zip(\n",
    "        df_results[MODEL_IDS[0]],\n",
    "        df_results[MODEL_IDS[1]],\n",
    "        df_results[MODEL_IDS[2]],\n",
    "        df_results[\"chroma\"],\n",
    "    )\n",
    "]\n",
    "print(sum(is_modeling_agree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_to_validate = []\n",
    "\n",
    "modeling_keywords = [\n",
    "    \"model\",\n",
    "    \"spatial\",\n",
    "    \"dynamics\",\n",
    "    \"forecast\",\n",
    "    \"simulate\",\n",
    "    \"simulating\",\n",
    "    \"quantify\",\n",
    "]\n",
    "modeling_titles = []\n",
    "titles = []\n",
    "\n",
    "for i, x in df_validate.iterrows():\n",
    "    if is_modeling_agree[i]:\n",
    "        titles.append(x[\"title\"])\n",
    "        for y in modeling_keywords:\n",
    "            if y in x[\"title\"].lower():\n",
    "                modeling_titles.append(x[\"title\"])\n",
    "\n",
    "print(len(titles))\n",
    "print(len(modeling_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in titles:\n",
    "    if x not in modeling_titles:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
