{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16edc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a4f0f",
   "metadata": {},
   "source": [
    "Set up the tokenizer and model. device_map=\"auto\" will automatically distribute the model across available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabd1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da1289",
   "metadata": {},
   "source": [
    "Show the model's chat capabiltiies. Note the inclusion of the custom tools section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc98985",
   "metadata": {},
   "source": [
    "Create a function to generate text using the model. The function takes a an array of chat messages and returns the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, tools=None) -> str:\n",
    "    input_tokens = tokenizer.apply_chat_template(\n",
    "        messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    inputs = {k: v for k, v in input_tokens.items()}\n",
    "\n",
    "    # generate_kwargs = {\"do_sample\": True, \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95}\n",
    "\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    response_tokens = output_tokens[0][input_tokens[\"input_ids\"].shape[-1] :]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76c171",
   "metadata": {},
   "source": [
    "Send the chat messages to the model. For our first tests, we won't be using any tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot that assists the user with research in infectious diseases and infectious disease modeling.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response = generate(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ede3e",
   "metadata": {},
   "source": [
    "We can not continue the conversation by capturing the response from the model in the chat messages array and adding a new user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98241aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response,\n",
    "    }\n",
    ")\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the mechanism of action of penicillin?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = generate(messages)\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response,\n",
    "    }\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffca0d7",
   "metadata": {},
   "source": [
    "We'll now define a function that the model can use as a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3593bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    return 22.0\n",
    "\n",
    "\n",
    "tools = [get_current_temperature]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc9867",
   "metadata": {},
   "source": [
    "We can now call generate with the tools, and the model will identify taht it needs to use a tool to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the current temperature in Paris, France?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = generate(messages, tools=tools)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b2977",
   "metadata": {},
   "source": [
    "We will add the tool call as well as the tool response to the chat messsage array. The model will then be able to continue the conversation with the tool response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1befd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# currently an issue with the tokenizer. the model returns 'parameters', but the tokenizer expects 'arguments'\n",
    "tool_call = json.loads(response)\n",
    "tool_call[\"arguments\"] = tool_call.pop(\"parameters\")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n",
    "\n",
    "## TODO call the function and get the results\n",
    "\n",
    "messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n",
    "\n",
    "response = generate(messages, tools=tools)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
